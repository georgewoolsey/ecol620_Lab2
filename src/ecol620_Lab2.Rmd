---
title: "Lab 2"
subtitle: "ECOL 620 - Applications in Landscape Ecology"
author: "George Woolsey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_float: true
    # code_folding: hide
editor_options: 
  chunk_output_type: console
knit: (function(inputFile, encoding){ 
    out_dir <- '../';
    rmarkdown::render(inputFile, encoding = encoding, output_file=file.path(dirname(inputFile), out_dir, 'index.html'));
    file.copy(from = "..//index.html", to = '..///data//lab2_george_woolsey.html', overwrite = TRUE)
  })
  
---

# Setup

```{r, include=FALSE, warning=F, message=F}
# knit options
knitr::opts_chunk$set(
  echo = TRUE
  , warning = FALSE
  , message = FALSE
  # , results='hide'
  , fig.width = 10
  , fig.height = 7
)
```

```{r, eval=T}
# bread-and-butter
library(tidyverse)
library(lubridate)
library(viridis)
library(scales)
library(latex2exp)
# visualization
library(ggrepel)
library(cowplot)
library(kableExtra)
library(GGally)
# spatial analysis
library(USAboundaries)
library(sf)
library(terra)
# set seed
set.seed(11)
```

# Rules

1. Using the provided R Markdown template, answer all questions and show your R code where necessary. Note, some questions are just plain text written answers.
2. Complete your assignment using the R markdown file and submit individual assignments to Canvas. Knit your script and submit an .html file on Canvas. Please use  the following naming convention: lab2_firstname_lastname.html (ex. lab2_kyle_horton.html).  Note, we will not grade labs in any other format.

# Question 1

Briefly describe (1-2 sentences) what each of the following functions achieve. All of these functions are seen throughout the Lab #2 example code. Use complete sentences (1 pt each). 

* `crop()` is from the `terra` package and is used to cut out a part of a raster based on another object or defined extent.
* `ext()` is from the `terra` package and is used to return or create the extent of a raster.
* `disagg()` - going from coarse to fine on raster, enhance the resolution (can do this by making all the same or use interpolation [e.g. distance weighted interpolation])
* `aggregate()` is from the `terra` package and is used to create a new raster with a lower spatial resolution (e.g. coarser grain size) by applying an aggregate function (e.g. mean).
* `global()` is from the `terra` package and is used to calculate summarized values (e.g. mean, sum) of the entire raster extent.
* `mask()` is from the `terra` package and is used to update the values of a raster to `NA` for specified cells (e.g. from a separate mask raster).
* `res()` is from the `terra` package and is used to get the resolution of a raster in terms of the grain (e.g. 30m$\times$30m).
* `st_buffer()` is from the `sf` package and is used to compute a buffer of a defined distance around a geometry.
* `rpois()` is used to draw a sample of $n$ from the Poisson distribution with parameter $\lambda$. 

# Question 2 

Create a 15 by 15 raster using the Poisson distribution to randomly draw the values. Design the raster to have a mean and variance of 5 (approximately). Generate a figure of the raster with cell values labeled (e.g., follow the aesthetic of Figure 2.5a, although with color). Include code to generate the raster and figure. For this example, use the supplied raster plotting code (don’t worry about ggplot, yet). Report the mean and variance of the newly generated raster. (3 pts)

```{r}
# generate raster
raster_size <- 15
raster_temp <- terra::rast(ncol = raster_size, nrow = raster_size, xmin = 0, xmax = raster_size, ymin = 0, ymax = raster_size)
# fill raster with random values
raster_temp[] <- rpois(n = terra::ncell(raster_temp), lambda = 5)
# plot raster
  # and report the mean and variance
raster_temp %>% as.data.frame(xy = TRUE) %>% 
  dplyr::rename(value = 3) %>% 
  ggplot(.) +
    geom_raster(
      mapping = aes(x = x, y = y, fill = value)
    ) +
    geom_text(
      mapping = aes(x = x, y = y, label = value)
      , color = "white"
    ) +
    scale_fill_viridis_c(option = "viridis") +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    labs(
      title = paste0(raster_size,"x",raster_size, " raster with values drawn from the Poisson distribution")
      , subtitle = paste0(
        "Mean: "
        , terra::global(raster_temp, fun = "mean")[[1]] %>% scales::comma(accuracy = .01)
        , ", Variance: "
        , terra::global(raster_temp, fun = "sd")[[1]]^2 %>% scales::comma(accuracy = .01)
      )
      , x = "x"
      , y = "y"
    ) +
    theme_bw() +
    theme(
      legend.position = "none"
      , axis.title = element_text(size = 8)
    )
```

<span style="color: teal;">
Based on the example raster created from random values drawn from a Poisson distribution with $\lambda = 5$, the mean cell value is **`r terra::global(raster_temp, fun = "mean")[[1]] %>% scales::comma(accuracy = .01)`** and the variance of the entire raster extent is is **`r terra::global(raster_temp, fun = "sd")[[1]]^2 %>% scales::comma(accuracy = .01)`**.
</span>

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

# Question 3

Within the “elevation” folder, there is a digital elevation raster of the southern Colorado areaAggregate (using the mean) the elevation raster by factors of 5, 30, 55, 80, …255 (i.e., by increments of 25). Plot the spatial grain versus spatial variance, using a log scale. Note, global won't calculate `var`, but will calculate `sd`. Describe the relationship between the two variables. (10 pts)

```{r}
dem <- terra::rast("../data/elevation/srtm_15_05.tif")
# define function
fn_agg_mean <- function(my_fact, my_data) {
  agg_temp <- terra::aggregate(my_data, fact = my_fact, fun = "mean")
  return(
    data.frame(
      agg_factor = my_fact
      , variance = terra::global(agg_temp, fun = "sd", na.rm = TRUE)[[1]]^2
      , xres = terra::yres(agg_temp)
      , yres = terra::yres(agg_temp)
    )
  )  
}
# create data frame of spatial variance vs grain using function defined above
df_grain_variance <- seq(from = 5, to = 255, by = 25) %>% # define sequence for factors
  purrr::map_dfr(fn_agg_mean, my_data = dem) # map = apply a function to each element of a list
# plot
ggplot(data = df_grain_variance) +
  geom_point(aes(x = xres, y = variance), size = 3, color = "gray30") +
  scale_y_log10() +
  scale_x_log10() +
  labs(
    y = "Log(spatial variance)"
    , x = "Log(grain)"
  ) +
  theme_bw() 
```


# Question 4

Include a plot of the elevation raster aggregated by a factor of 55 using ggplot. Please include a state overlay (see code from Lab #1 for state data, `states_map <- map_data("state")`). If you use a projection, ggplot will take longer to process the image. Re-projecting the data is not necessary for this exercise. (8 pts) 


```{r}
# aggregate dem using factor of 55
df_dem_temp <- terra::aggregate(dem, fact = 55, fun = "mean") %>%
  # convert to data frame
  as.data.frame(xy = TRUE) %>% 
  dplyr::rename(value = 3)
# get list of states that overlap the dem
state_list <- USAboundaries::us_states() %>% 
  sf::st_crop(terra::ext(dem)) %>% 
  sf::st_set_geometry(NULL) %>% 
  dplyr::pull(var = stusps)
# plot
ggplot() +
  geom_raster(
    data = df_dem_temp
    , mapping = aes(x = x, y = y, fill = value)
  ) +
  geom_sf(
    data = USAboundaries::us_counties(states = state_list)
    , alpha = 0, lwd = 0.5, color = "black"
  ) +
  geom_sf(
    data = USAboundaries::us_states(states = state_list)
    , alpha = 0, lwd = 1, color = "black"
  ) +
  scale_fill_viridis_c(
    option = "cividis"
    , alpha = 0.9
    , na.value = "transparent"
    , breaks = scales::extended_breaks(n = 6)
    , labels = scales::comma
  ) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  labs(
    title = "Elevation map using DEM"
    , subtitle = paste0(sort(state_list), collapse = ", ")
    , x = "Longitude"
    , y = "Latitude"
    , fill = "Elev. (m)"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom"
    , legend.direction = "horizontal"
    , legend.margin = margin(0,0,0,0)
    , legend.box.margin = margin(-7,0,2,0)
    , legend.text = element_text(size = 7, angle = 25, hjust = 0.7)
    , legend.title = element_text(size = 7)
    , axis.title = element_text(size = 7)
    , axis.text.y = element_text(size = 7)
    , axis.text.x = element_text(size = 7, angle = 35, hjust = 0.7)
    , panel.border = element_blank()
    , plot.subtitle = element_text(size=9, face="italic")
  )
```

<span style="color: teal;">
Note that the provided DEM overlaps portions of **`r sort(state_list)`** as shown in the map above.
</span>

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls())
gc()
```

# Five-lined Skink Data (multi-scale analysis)

## Question 5

Once you have cropped raster “nlcd” using a 10km barrier from the min and max coordinates of the reptile sampling locations (Lines 245-249), how many cells remain? What percent reduction did you see from the original raster?  (3 pts)

```{r, results='hide', warning=FALSE, message=FALSE}
# load nlcd raster
nlcd <- terra::rast("../data/nlcd2011SE/nlcd2011SE.tif") %>% 
  as.factor()
# load site and reptile data
sites <- sf::st_read("../data/reptiledata/reptiledata.shp") %>% 
  sf::st_set_crs(., value = terra::crs(nlcd))
# create custom color palette
my_col <- c("black","blue","darkorange","red","darkred","grey30","grey50", "lightgreen",
            "green", "darkgreen", "yellow", "goldenrod", "purple", "orchid","lightblue", "lightcyan")
# crop nlcd to site data + buffer
my_buffer_m_temp <- 10000 # 10000 = 10,000m = 10km
nlcd_crop <- nlcd %>% 
  terra::crop(terra::ext(
    sf::st_bbox(sites)[1] - my_buffer_m_temp # xmin
    , sf::st_bbox(sites)[3] + my_buffer_m_temp # xmax
    , sf::st_bbox(sites)[2] - my_buffer_m_temp # ymin
    , sf::st_bbox(sites)[4] + my_buffer_m_temp # ymax
  )) 
# calculate cell reduction
cell_reduction <- terra::ncell(nlcd) - terra::ncell(nlcd_crop)
pct_cell_reduction <- (terra::ncell(nlcd_crop) - terra::ncell(nlcd)) / terra::ncell(nlcd)
```

<span style="color: teal;">
The original NLCD raster contained **`r terra::ncell(nlcd) %>% scales::comma(accuracy = 1)`** cells. The NLCD raster cropped to the survey site data extent with a 10km buffer contains **`r terra::ncell(nlcd_crop) %>% scales::comma(accuracy = 1)`** cells. The number of cells decreased by **`r cell_reduction %>% scales::comma(accuracy = 1)`** for a **`r pct_cell_reduction %>% scales::percent(accuracy = 0.1)`** change.
</span>

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

## Question 6

Using intervals of 500 m and a maximum range of 5000 m (minimum range of 500 m), where do you see the greatest correlation in each pairwise combination of scales? Is this surprising? Plot the scatter plot of the greatest pair-wise correlation combination. See Figure 2.9 for an example of many similar pairwise plots. (5 pts)

### Data Preparation

```{r}
# get nlcd classes
nlcd_classes <- levels(nlcd_crop)[[1]]
# define forest 0/1 based on nlcd class
  # 41, 42, and 43 (conifer, deciduous and mixed forests)
nlcd_classes_forest <- as.numeric(nlcd_classes %in% c(41,42,43))
#create reclassify matrix: first col: original; second: change to
nlcd_reclass_matrix <- cbind(nlcd_classes, nlcd_classes_forest)
#reclassify with classify function is faster
forest_crop <- terra::classify(nlcd_crop, nlcd_reclass_matrix)
# define function to calculate pct forest cover at one site
fn_get_site_fcov <- function(my_site_row, hey_buffer_m){
  return(
    data.frame(
      site = sites$site[my_site_row]
      , buffer_m = hey_buffer_m
      , pct_forest_cover = forest_crop %>% 
          terra::crop(
            sites[my_site_row,] %>% 
              sf::st_buffer(dist = hey_buffer_m)
          ) %>% 
          terra::global(fun = "mean", na.rm = TRUE) %>% 
          as.numeric()
    )
  )
}
# define function to apply to all rows of data set
fn_get_df_fcov <- function(my_buffer_m) {
  c(1:nrow(sites)) %>% 
    purrr::map_dfr(fn_get_site_fcov, hey_buffer_m = my_buffer_m)
}
# apply to sequence list to return data frame
df_buffer_fcov <- c(500, seq(from = 1000, to = 5000, by = 1000)) %>% 
  purrr::map_dfr(fn_get_df_fcov)
# tidyr::pivot_wider for plot using GGally::ggpairs and correlation matrix
df_buffer_fcov_wide <- df_buffer_fcov %>% 
  dplyr::mutate(
    pct_forest_cover = pct_forest_cover*100
    , buffer_name = ifelse(
        buffer_m < 1000
        , paste0(buffer_m, "m")
        , paste0(buffer_m/1000, "km")
      )
  ) %>% 
  dplyr::select(-c(buffer_m)) %>% 
  tidyr::pivot_wider(
    names_from = buffer_name
    , values_from = pct_forest_cover
    , names_glue = "buffer_{buffer_name}"
  )
# calculate correlation matrix
  cor_matrix <- df_buffer_fcov_wide %>% dplyr::select(-c(site)) %>% 
    cor()
  # keep lower triangle
  cor_matrix[!lower.tri(cor_matrix, diag = FALSE)] <- NA
  cor_matrix <- cor_matrix[2:nrow(cor_matrix), 1:ncol(cor_matrix)-1]
  # find the highest correlation
  max_cor_temp <- max(cor_matrix, na.rm = TRUE)
  k_temp <- arrayInd(which(cor_matrix == max_cor_temp), dim(cor_matrix))
  highest_cor_names <- mapply(`[[`, dimnames(cor_matrix), k_temp)
  # rename rows and cols
  rownames(cor_matrix) <- rownames(cor_matrix) %>% 
    stringr::str_replace_all("[[:punct:]]", " ") %>% 
    stringr::word(2)
  colnames(cor_matrix) <- colnames(cor_matrix) %>% 
    stringr::str_replace_all("[[:punct:]]", " ") %>% 
    stringr::word(2)
```

### Correlation Matrix

```{r}
# HTML table of correlation matrix
options(knitr.kable.NA = "")
kableExtra::kable(cor_matrix
    , format = "html" 
    , caption = "Correlation between forest cover at different buffer sizes"
    , digits = 3
  ) %>% 
  kable_styling(font_size = 11)
```

### Plot of highest correlation buffer combination

```{r}
# plot only highest correlation buffers
ggplot(data = df_buffer_fcov_wide) + 
  geom_point(
    mapping = aes_string(y = highest_cor_names[1], x = highest_cor_names[2])
    , color = "gray30", size = 3
  ) +
  labs(
    title = "Scatter plot of the greatest pair-wise correlation buffer combination"
    , subtitle = paste0(paste0(
        highest_cor_names %>% 
          stringr::str_replace_all("[[:punct:]]", " ") %>% 
          stringr::word(2)
        , collapse = " & "
      ), " correlation = ", round(max_cor_temp,3))
    , y = paste0(
        "Forest cover surrounding sample location at "
        , highest_cor_names[1] %>% 
            stringr::str_replace_all("[[:punct:]]", " ") %>% 
            stringr::word(2)
        , " (%)"
      )
    , x = paste0(
        "Forest cover surrounding sample location at "
        , highest_cor_names[2] %>% 
            stringr::str_replace_all("[[:punct:]]", " ") %>% 
            stringr::word(2)
        , " (%)"
      )
  ) +
  theme_bw() + 
  theme(
    axis.title = element_text(size = 9)
  )
  
```

### Recreate Figure 2.9 matrix of pairwise plots

See [Fletcher and Fortin (2018)](https://link.springer.com/book/10.1007/978-3-030-01989-1) Figure 2.9 on page 39

```{r}
# plot with GGally::ggpairs
my_vars <- names(df_buffer_fcov_wide %>% dplyr::select(-c("site")))
plt_g <- GGally::ggpairs(
    data = df_buffer_fcov_wide
    , columns = my_vars
    , columnLabels = my_vars %>% stringr::str_replace_all("[[:punct:]]", " ") %>% stringr::word(2)
    # , upper = "blank"
    , diag = "blank"
    , xlab = "Forest cover surrounding sampling locations (%)"
    , ylab = "Forest cover surrounding sampling locations (%)"
    , switch = "both"
  ) +
  theme_bw()
# print  plot
plt_g
```

### Written Answer

Where do you see the greatest correlation in each pairwise combination of scales? Is this surprising?

<span style="color: teal;">
The greatest correlation in each pairwise combination of scales occurs between the buffers of `r paste0(paste0(highest_cor_names %>% stringr::str_replace_all("[[:punct:]]", " ") %>% stringr::word(2), collapse= " & "), " correlation = ", round(max_cor_temp,3))`. This is not surprising that the greatest correlation occurs between the largest buffer sizes considered because as the spatial extent is increased, while holding grain constant, we are effectively increasing our sample size. As we increase the sample size by increasing the buffer, we expect that the sample mean will converge on the population mean (full raster extent).
</span>


```{r, include=FALSE, eval=FALSE}
# view forest cover in 500m buffer of survey site 1
forest_crop %>% 
  terra::crop(sites %>% dplyr::slice(n=1) %>% sf::st_buffer(dist = 500)) %>% 
  as.data.frame(xy = TRUE) %>% 
  dplyr::rename(is_forest = 3) %>% 
  dplyr::mutate(is_forest = as.factor(is_forest)) %>% 
  ggplot(data=.) +
    geom_raster(mapping=aes(x=x, y=y, fill=is_forest)) + 
    geom_sf(data = sites %>% dplyr::slice(n=1), size = 3) +
    geom_sf_label(
      data = sites %>% dplyr::slice(n=1)
      , aes(label=site)
      , vjust = 1.3
    ) +
    scale_fill_manual(values = c("gray90", "forestgreen")) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    labs(title="Forest cover in 500m buffer of example survey site", fill = "is forest", x="", y="") +
    theme_bw() +
    theme(
      legend.position = "top"
      , legend.direction = "horizontal"
    )


# seq(from = 500, to = 5000, by = 500)
#Hint:
#ggplot()+
#  geom_point()+
#  theme_classic()+
#  labs(y = "Forest cover surrounding sample location at XXkm (%)", x = "Forest cover surrounding sample   location at XXkm (%))")
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

## Question 7

Generate Figure 2.11 (only panels a and b) using ggplot. Use 500 m intervals and a maximum range of 5000 m (minimum range of 500 m). What does this suggest about the scale of drivers of occurrence for five-lined skinks? Where is the strongest association? How does your conclusion differ (or not) from the results of Fletcher and Fortin? Do you believe the scale of effect has been captured by the scales sampled? Be sure to add appropriate labels. (12 pts) 


```{r}
# load the reptile data on southeastern five-lined skinks (FLSK) and 
flsk <- read.csv("../data/reptiledata/reptiles_flsk.csv", header=T)
# merge it to our summaries of forest cover calculated at different scales
df_buffer_fcov <- df_buffer_fcov %>% 
  dplyr::mutate(pct_forest_cover_int = pct_forest_cover*100) %>% 
  dplyr::left_join(flsk, by = c("site" = "site"))
# function to run logistic regression of presence on forest cover
fn_logistic_buffers <- function(my_buffer_m) {
  glm_temp <- glm(
    data = df_buffer_fcov %>% dplyr::filter(buffer_m == my_buffer_m)
    , formula = pres ~ pct_forest_cover_int
    , family = "binomial"
  )
  # return data frame
  return(
    data.frame(
      buffer_m = my_buffer_m
      , beta = coef(glm_temp)[2] %>% as.numeric
      , log_likelihood = logLik(glm_temp) %>% as.numeric
      # just the beta CI's
      , lower = confint(glm_temp)[2,1]
      , upper = confint(glm_temp)[2,2]
    )
  )
}
# run logistic regression for all buffer sizes
df_logistic <- df_buffer_fcov$buffer_m %>% unique() %>% 
  purrr::map_dfr(fn_logistic_buffers)
# create plot for log-likelihood
  # The log-likelihood for logistic regression models, where the probability 
  # of skink occurrence is modeled as a function of the percent forest, calculated 
  # at different scales using a circular buffer around points (0.5–5 km). 
  # A higher log-likelihood suggests a better fit of the model to the data.
plt_ll <- ggplot(data = df_logistic, mapping = aes(x = buffer_m, y = log_likelihood)) +
  geom_line() + 
  geom_point(shape = 21, color = "black", fill = "gray", size = 3) +
  scale_x_continuous(labels = scales::comma) +
  labs(
    x = "Forest cover scale (m)"
    , y = "Log-likelihood"
  ) +
  theme_bw() +
  theme(
    legend.position = "none"
    , axis.title = element_text(size = 9)
  )
# create plot for beta
  # The parameter estimates for the effect of forest cover on 
  # the probability of skink occurrence using different sized buffers. 
  # Note the similarity in estimates among scales.
plt_beta <- ggplot(data = df_logistic, mapping = aes(x = buffer_m)) +
  geom_errorbar(mapping = aes(ymin = lower, ymax = upper), color = "black", width = 0.1) + 
  geom_point(mapping = aes(y = beta), shape = 21, color = "black", fill = "white", size = 3) +
  scale_x_continuous(labels = scales::comma) +
  labs(
    x = "Forest cover scale (m)"
    , y = latex2exp::TeX("$\\beta$ (95% CI)")
  ) +
  theme_bw() +
  theme(
    legend.position = "none"
    , axis.title = element_text(size = 9)
  )
# combine plots
cowplot::plot_grid(
  plotlist =  list(plt_ll, plt_beta)
  , nrow = 1
)
```

### Written Answer

What does this suggest about the scale of drivers of occurrence for five-lined skinks? Where is the strongest association? How does your conclusion differ (or not) from the results of Fletcher and Fortin? Do you believe the scale of effect has been captured by the scales sampled? Be sure to add appropriate labels.

<span style="color: teal;">
The scale with the best fit to the data (e.g., highest log-likelihood) based on the buffers sizes considered in this analysis is at **`r df_logistic %>% dplyr::filter(log_likelihood == max(df_logistic$log_likelihood)) %>% dplyr::pull(var = buffer_m) %>% scales::comma()`m**. This means that, based on the log-likelihoods of the models considered, forest cover within **`r df_logistic %>% dplyr::filter(log_likelihood == max(df_logistic$log_likelihood)) %>% dplyr::pull(var = buffer_m) %>% scales::comma()`m** is most supported by the data for determining the presence of five-lined skinks. This conclusion differs with the results of [Fletcher and Fortin (2018)](https://link.springer.com/book/10.1007/978-3-030-01989-1) who determined the "scale of the effect" for this data at 2,000m, presumably using the same data. It is possible that this disagreement is due to different data vintages between the two analyses; otherwise, the differences are likely due to methodology. Based on this analysis, I do not believe that the scale of effect has been adequately captured by the scales sampled. The highest log-likelihood occurred at the greatest buffer size considered and it is possible that even greater buffer sizes not considered in this analysis would have had greater support based on the data.
</span>
